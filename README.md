# ğŸš€ RAG Document Analyzer

A **production-ready Retrieval-Augmented Generation (RAG) system** for intelligent document question-answering, powered by **Groqâ€™s ultra-fast LLM inference**.

ğŸ¥ **Live Demo**: https://rag-document-analyzer.vercel.app  
ğŸ“– **API Docs (Swagger)**: https://rag-document-analyzer.onrender.com/docs  
ğŸ› **Report Bug**: https://github.com/Emart29/rag-document-analyzer/issues  

[![Live Demo](https://img.shields.io/badge/Live-Demo-green)](https://rag-document-analyzer.vercel.app)
[![API Docs](https://img.shields.io/badge/API-Docs-blue)](https://rag-document-analyzer.onrender.com/docs)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## âœ¨ Overview

**RAG Document Analyzer** allows users to upload PDF documents and ask natural-language questions while receiving **accurate, source-cited answers**.  
It combines **semantic search + LLM reasoning** for fast, reliable document intelligence.

Built with:
- **React + Vite** (Frontend)
- **FastAPI** (Backend)
- **Groq (Llama 3.1 70B)** for lightning-fast inference
- **ChromaDB** for vector similarity search

---

## âœ¨ Features

### ğŸ¯ Core Capabilities
- **ğŸ“„ PDF Document Processing**: Upload and process PDFs with intelligent text chunking
- **ğŸ§  AI-Powered Q&A**: Ask natural language questions and get accurate answers
- **ğŸ” Semantic Search**: Vector-based similarity search using ChromaDB
- **ğŸ“š Source Citations**: Every answer includes relevant document excerpts with page numbers
- **ğŸ’¬ Conversation History**: Maintains context across multiple questions
- **âš¡ Lightning Fast**: Powered by Groq's ultra-fast LLM inference (500+ tokens/sec)

### ğŸ¨ Modern UI/UX
- **Beautiful React Interface**: Built with Vite, TailwindCSS, and shadcn/ui
- **Responsive Design**: Works seamlessly across devices
- **Real-time Updates**: Live upload progress and query processing
- **Dark Mode Support**: Eye-friendly interface (if implemented)
- **Drag & Drop**: Intuitive document upload experience

### ğŸ› ï¸ Technical Excellence
- **Production-Ready**: Comprehensive error handling and validation
- **Scalable Architecture**: Modular, maintainable codebase
- **API Documentation**: Auto-generated Swagger/OpenAPI docs
- **Type Safety**: Pydantic models for request/response validation
- **Performance Optimized**: Efficient chunking, batching, and caching

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     React Frontend (Vite)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Document    â”‚  â”‚     Chat     â”‚  â”‚   Source     â”‚      â”‚
â”‚  â”‚   Upload     â”‚  â”‚  Interface   â”‚  â”‚  Citations   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ REST API (Axios)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Backend                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              RAG Engine (Orchestrator)                â”‚   â”‚
â”‚  â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚     â”‚            â”‚            â”‚            â”‚                â”‚
â”‚  â”Œâ”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ PDF  â”‚   â”‚ Groq  â”‚   â”‚Sentenceâ”‚   â”‚ Chroma  â”‚         â”‚
â”‚  â”‚Procesâ”‚   â”‚ API   â”‚   â”‚Transformâ”‚   â”‚   DB    â”‚         â”‚
â”‚  â”‚sor   â”‚   â”‚(LLM)  â”‚   â”‚ (Embed)â”‚   â”‚(Vector) â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.8+**
- **Node.js 18+**
- **Groq API Key** ([Get free key](https://console.groq.com/))
- **8GB RAM minimum**
- **2GB free disk space**

### Installation

#### 1ï¸âƒ£ Clone Repository
```bash
git clone https://github.com/Emart29/rag-document-analyzer.git
cd rag-document-analyzer
```

#### 2ï¸âƒ£ Setup Backend
```bash
# Navigate to backend
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env and add your GROQ_API_KEY

# Start backend server
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

Backend will start on: **http://localhost:8000**

API Documentation: **http://localhost:8000/docs**

#### 3ï¸âƒ£ Setup Frontend
```bash
# Open new terminal, navigate to frontend
cd frontend

# Install dependencies
npm install

# Start development server
npm run dev
```

Frontend will start on: **http://localhost:5173**

---

## ğŸ“– Usage

### Upload Documents
1. Click the **Upload** tab in the sidebar
2. Drag & drop a PDF or click **Browse**
3. Wait for processing (10-30 seconds)
4. Document appears in the **Documents** list

### Ask Questions
1. Select documents (optional - searches all if none selected)
2. Type your question in the chat input
3. Press **Enter** or click **Send**
4. Get AI-generated answer with source citations in 2-3 seconds

### Example Questions
```
"What are the main findings of this research?"
"Summarize the key points from page 5"
"Compare the methodologies discussed in the documents"
"What does the author say about machine learning?"
```

---

## ğŸ› ï¸ Technology Stack

### Frontend
| Technology | Purpose |
|------------|---------|
| **React 18** | UI framework |
| **Vite** | Build tool & dev server |
| **TailwindCSS** | Styling |
| **shadcn/ui** | UI components |
| **Tanstack Query** | Server state management |
| **Axios** | HTTP client |
| **React Markdown** | Answer rendering |
| **Lucide React** | Icons |

### Backend
| Technology | Purpose |
|------------|---------|
| **FastAPI** | REST API framework |
| **Groq API** | LLM inference (Llama 3.1 70B) |
| **ChromaDB** | Vector database |
| **Sentence Transformers** | Text embeddings |
| **PyPDF2 & pdfplumber** | PDF text extraction |
| **Pydantic** | Data validation |
| **Uvicorn** | ASGI server |

---

## ğŸ“Š Performance Metrics

**Tested on:** Dell Latitude 5400 (i5-8th gen, 8GB RAM)

| Operation | Time | Notes |
|-----------|------|-------|
| **Document Upload** | 10-30s | 5-page PDF |
| **Question Answering** | 1-3s | Including retrieval & generation |
| **Semantic Search** | <100ms | ChromaDB query |
| **Memory Usage** | ~600MB | Backend runtime |
| **LLM Inference** | 500+ tokens/sec | Via Groq |

---

## ğŸš¢ Deployment

### Frontend Deployment (Vercel) - FREE âœ…

```bash
# Install Vercel CLI
npm i -g vercel

# Navigate to frontend folder
cd frontend

# Deploy
vercel

# Follow prompts, set environment variable:
# VITE_API_URL=https://your-backend-url.onrender.com
```

### Backend Deployment (Render.com) - FREE âœ…

1. Create `render.yaml` in project root:
```yaml
services:
  - type: web
    name: rag-backend
    env: python
    region: oregon
    plan: free
    buildCommand: |
      cd backend
      pip install -r requirements.txt
    startCommand: cd backend && uvicorn app.main:app --host 0.0.0.0 --port $PORT
    envVars:
      - key: GROQ_API_KEY
        sync: false
      - key: PYTHON_VERSION
        value: 3.11.0
```

2. Push to GitHub
3. Connect repo to Render.com
4. Add `GROQ_API_KEY` environment variable
5. Deploy!

**Your app will be live at:**
- Frontend: `https://your-app.vercel.app`
- Backend API: `https://your-api.onrender.com`

---

## ğŸ“ Project Structure

```
rag-document-analyzer/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â”‚   â”œâ”€â”€ models.py            # Pydantic models
â”‚   â”‚   â”œâ”€â”€ routers/             # API endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ documents.py     # Document operations
â”‚   â”‚   â”‚   â”œâ”€â”€ query.py         # Q&A endpoints
â”‚   â”‚   â”‚   â””â”€â”€ system.py        # Health & stats
â”‚   â”‚   â”œâ”€â”€ services/            # Business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ rag_engine.py    # RAG orchestration
â”‚   â”‚   â”‚   â”œâ”€â”€ groq_client.py   # LLM interface
â”‚   â”‚   â”‚   â”œâ”€â”€ embeddings.py    # Vector generation
â”‚   â”‚   â”‚   â””â”€â”€ pdf_processor.py # Document processing
â”‚   â”‚   â””â”€â”€ database/
â”‚   â”‚       â””â”€â”€ chroma_db.py     # Vector database
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ .env
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/          # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ Header.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ Sidebar.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatInterface.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ DocumentList.jsx
â”‚   â”‚   â”‚   â””â”€â”€ ui/              # shadcn components
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.js           # API client
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â””â”€â”€ main.jsx
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ README.md                    # This file
â””â”€â”€ LICENSE
```

---

## ğŸ§ª Testing

### Backend Tests
```bash
cd backend

# Test individual components
python app/services/groq_client.py
python app/services/embeddings.py
python app/database/chroma_db.py
python app/services/rag_engine.py

# API documentation
# Visit: http://localhost:8000/docs
```

### Frontend Tests
```bash
cd frontend

# Run development server
npm run dev

# Build for production
npm run build

# Preview production build
npm run preview
```

---

## ğŸ”’ Security & Best Practices

- âœ… Environment variables for sensitive data
- âœ… Input validation with Pydantic
- âœ… CORS configuration for frontend
- âœ… File size and type validation
- âœ… Error handling and logging
- âœ… API rate limiting (recommended for production)
- âš ï¸ Add authentication for production use
- âš ï¸ Implement user management for multi-tenant

---

## ğŸ› Troubleshooting

### Backend Issues

**Issue:** "GROQ_API_KEY not found"
```bash
# Solution: Check .env file
cat backend/.env  # Should show GROQ_API_KEY=gsk_...
```

**Issue:** ChromaDB errors
```bash
# Solution: Clear database and restart
rm -rf backend/chroma_db/
# Restart backend
```

**Issue:** PDF processing fails
```bash
# Solution: Install additional dependencies
pip install python-magic-bin  # Windows
pip install python-magic       # Mac/Linux
```

### Frontend Issues

**Issue:** API connection refused
```bash
# Solution: Check VITE_API_URL in .env
# Verify backend is running on correct port
```

**Issue:** Build errors
```bash
# Solution: Clear cache and reinstall
rm -rf node_modules package-lock.json
npm install
```

---

## ğŸ¯ Future Enhancements

- [ ] **Multi-format Support**: Add .docx, .txt, .md support
- [ ] **User Authentication**: JWT-based auth system
- [ ] **Multi-tenancy**: Per-user document isolation
- [ ] **Advanced Filters**: Filter by date, size, type
- [ ] **Export Conversations**: Download chat history
- [ ] **Streaming Responses**: Real-time token streaming
- [ ] **Voice Input**: Speech-to-text for questions
- [ ] **Document Summarization**: Auto-generate summaries
- [ ] **Collaborative Sharing**: Share documents between users
- [ ] **Analytics Dashboard**: Usage statistics and insights

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¤ Author

**Your Name**

- LinkedIn: [Emmanuel Nwanguma](https://linkedin.com/in/nwangumaemmanuel)
- GitHub: [Emart29](https://github.com/Emart29)

---

## ğŸ™ Acknowledgments

- [Groq](https://groq.com/) - Ultra-fast LLM inference platform
- [ChromaDB](https://www.trychroma.com/) - Open-source embedding database
- [FastAPI](https://fastapi.tiangolo.com/) - Modern Python web framework
- [Sentence Transformers](https://www.sbert.net/) - State-of-the-art text embeddings
- [shadcn/ui](https://ui.shadcn.com/) - Beautiful React components
- [Vercel](https://vercel.com/) - Frontend hosting platform
- [Render](https://render.com/) - Backend hosting platform

---

## ğŸ“Š Project Stats

![GitHub stars](https://img.shields.io/github/stars/Emart29/rag-document-analyzer?style=social)
![GitHub forks](https://img.shields.io/github/forks/Emart29/rag-document-analyzer?style=social)
![GitHub issues](https://img.shields.io/github/issues/Emart29/rag-document-analyzer)
![GitHub pull requests](https://img.shields.io/github/issues-pr/Emart29/rag-document-analyzer)

---

<div align="center">

**â­ If this project helped you, please give it a star! â­**

Made with â¤ï¸ by [Emmanuel Nwanguma](https://github.com/Emart29)

[â¬† Back to Top](#-rag-document-analyzer)

</div>
---

## ğŸ“ˆ Project 3: LLM Observability & Monitoring

This project now includes a fully integrated observability layer across the FastAPI backend and React frontend dashboard.

### âœ… Implemented Features

- **Custom Logging for All LLM Calls**
  - Every Groq request is wrapped and logged with prompt input, answer output, timestamps, model, metadata, status, and errors.
- **Prompt & Response Tracking (Persistent DB)**
  - Stored in `llm_request_logs` using SQLAlchemy with SQLite by default (`observability.db`) or PostgreSQL via `OBSERVABILITY_DB_URL`.
- **Token Usage Monitoring**
  - Captures prompt tokens, completion tokens, and total tokens for each LLM call.
- **Cost per Query Calculation**
  - Uses configurable per-1K token rates:
    - `GROQ_INPUT_TOKEN_COST_PER_1K`
    - `GROQ_OUTPUT_TOKEN_COST_PER_1K`
- **Latency Tracking**
  - Measures and stores per-request LLM latency in milliseconds.
- **Prompt Versioning**
  - Prompt templates are versioned in `prompt_templates` with active/inactive states for auditability and experimentation.
- **Dashboard Visualization (React)**
  - New Observability tab visualizes:
    - total queries
    - total tokens
    - total cost
    - average latency
    - daily trends
    - active prompt templates
    - recent LLM request logs

### ğŸ—‚ï¸ Observability API Endpoints

- `GET /observability/metrics?window_hours=24`
- `GET /observability/logs?limit=50`
- `GET /observability/prompts`
- `POST /observability/prompts`

### âš™ï¸ Environment Variables

Add these to `backend/.env`:

```bash
# Observability database (SQLite default)
OBSERVABILITY_DB_URL=sqlite:///./observability.db

# Optional: PostgreSQL example
# OBSERVABILITY_DB_URL=postgresql+psycopg2://user:password@localhost:5432/rag_observability

# Cost model (USD per 1000 tokens)
GROQ_INPUT_TOKEN_COST_PER_1K=0.00059
GROQ_OUTPUT_TOKEN_COST_PER_1K=0.00079
```

### ğŸ§© Code Structure Added for Project 3

```text
backend/app/
â”œâ”€â”€ database/
â”‚   â””â”€â”€ observability_db.py          # SQLAlchemy engine + tables
â”œâ”€â”€ services/
â”‚   â””â”€â”€ observability_service.py     # logging, metrics, prompt versions
â”œâ”€â”€ routers/
â”‚   â””â”€â”€ observability.py             # observability REST endpoints
â”œâ”€â”€ services/groq_client.py          # wrapped LLM calls + observability capture
â””â”€â”€ models.py                        # observability response models

frontend/src/
â”œâ”€â”€ components/
â”‚   â””â”€â”€ ObservabilityDashboard.jsx   # metrics dashboard UI
â”œâ”€â”€ services/
â”‚   â””â”€â”€ api.js                       # observability API methods
â””â”€â”€ App.jsx                          # Chat + Observability tabs
```

### â–¶ï¸ How to Run Project 3 End-to-End

1. Start backend:
```bash
cd backend
pip install -r requirements.txt
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

2. Start frontend:
```bash
cd frontend
npm install
npm run dev
```

3. Use the app:
- Upload a PDF
- Ask questions in Chat
- Open the **Observability** tab to see metrics and request logs in real time

### ğŸ¯ Benefits

- Better production debugging and reliability
- Cost and token transparency per query
- Prompt governance and audit history
- Easier performance optimization through latency and usage trends

### ğŸ“£ Suggested Social Media Lines

- "Just shipped full LLM observability in my RAG Document Analyzer: token tracking, query cost, latency metrics, prompt versioning, and a live dashboard. #LLMOps #RAG #FastAPI #React"
- "Project 3 complete âœ… Added production-grade monitoring to a RAG app with Groq + ChromaDB: logs, usage analytics, prompt audits, and metrics dashboard. #GenAI #MLOps"
- "From prototype to production: integrated end-to-end LLM monitoring (prompts, responses, tokens, cost, latency) into our document Q&A platform. #LLMObservability"
